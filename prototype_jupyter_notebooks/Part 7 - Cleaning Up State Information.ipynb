{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# std imports\n",
    "import copy\n",
    "import math\n",
    "import re\n",
    "\n",
    "# installed package imports\n",
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pyzbar import pyzbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rotation_estimator:\n",
    "    def __init__(self):\n",
    "        self.theta = [0.0,0.0,0.0]\n",
    "        self.alpha = 0.98\n",
    "        self.first = True\n",
    "        self.last_ts_gyro = 0.0\n",
    "    \n",
    "    # Function to calculate the change in angle of motion based on data from gyro\n",
    "    # gyro_data - list of 3 elems\n",
    "    # ts - float arrival time of curr gyro frame\n",
    "    def process_gyro(self, gyro_data, ts):\n",
    "        if self.first:\n",
    "            self.last_ts_gyro = ts\n",
    "            return\n",
    "        \n",
    "        gyro_angle = [gyro_data.x,gyro_data.y,gyro_data.z]\n",
    "        \n",
    "        dt_gyro = (ts-self.last_ts_gyro)/1000.0\n",
    "        self.last_ts_gyro = ts\n",
    "        \n",
    "        gyro_angle = [gyro_angle[0]*dt_gyro,gyro_angle[1]*dt_gyro,gyro_angle[2]*dt_gyro]\n",
    "        \n",
    "        self.theta = [self.theta[0]-gyro_angle[2],self.theta[1]-gyro_angle[1],self.theta[2]+gyro_angle[0]]\n",
    "    \n",
    "    # Function to calculate the change in angle of motion based on data from accelerometer\n",
    "    # accel_data - list of 3 elems\n",
    "    def process_accel(self, accel_data):\n",
    "        accel_angle = [0.0,0.0,0.0]\n",
    "        accel_angle[2] = np.arctan2(accel_data.y,accel_data.z)\n",
    "        accel_angle[0] = np.arctan2(accel_data.x, np.sqrt(accel_data.y**2+accel_data.z**2))\n",
    "        \n",
    "        if self.first:\n",
    "            self.first = False\n",
    "            self.theta = accel_angle\n",
    "            self.theta[1] = np.pi/2\n",
    "        else:\n",
    "            self.theta[0] = self.theta[0]*self.alpha + accel_angle[0]*(1-self.alpha)\n",
    "            self.theta[2] = self.theta[2]*self.alpha + accel_angle[2]*(1-self.alpha)\n",
    "        \n",
    "    def get_theta(self):\n",
    "        theta_out = [0.0,0.0,0.0]\n",
    "        theta_out = [self.theta[0]*180/np.pi, self.theta[1]*180/np.pi, (self.theta[2]-np.pi/2)*180/np.pi]\n",
    "        return theta_out\n",
    "    \n",
    "def get_robot_euler_angle(frames):\n",
    "    accel_data = gyro_data = gyro_ts = None\n",
    "    \n",
    "    accel_frame = frames.first_or_default(rs.stream.accel)\n",
    "    if accel_frame:\n",
    "        accel_data = accel_frame.as_motion_frame().get_motion_data()\n",
    "\n",
    "    gyro_frame = frames.first_or_default(rs.stream.gyro)\n",
    "    if gyro_frame:\n",
    "        gyro_data = gyro_frame.as_motion_frame().get_motion_data()\n",
    "        gyro_ts = gyro_frame.as_motion_frame().get_timestamp()\n",
    "    \n",
    "    return accel_frame, accel_data, gyro_frame, gyro_data, gyro_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure depth and color streams\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30) # depth stream\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30) # color stream\n",
    "config.enable_stream(rs.stream.gyro) # gyro stream\n",
    "config.enable_stream(rs.stream.accel) # accelerometer stream\n",
    "align = rs.align(rs.stream.color) # align both streams to same pov\n",
    "\n",
    "# Enable visualizer and filters for later use\n",
    "colorizer = rs.colorizer()\n",
    "spatial = rs.spatial_filter()\n",
    "spatial.set_option(rs.option.filter_magnitude, 5)\n",
    "spatial.set_option(rs.option.filter_smooth_alpha, 1)\n",
    "spatial.set_option(rs.option.filter_smooth_delta, 50)\n",
    "spatial.set_option(rs.option.holes_fill, 3)\n",
    "hole_filling = rs.hole_filling_filter()\n",
    "depth_to_disparity = rs.disparity_transform(True)\n",
    "disparity_to_depth = rs.disparity_transform(False)\n",
    "\n",
    "# Image detection size\n",
    "expected = 300\n",
    "inScaleFactor = 0.007843\n",
    "meanVal = 127.53\n",
    "\n",
    "net = cv2.dnn.readNetFromTensorflow('frozen_inference_graph.pb', 'graph.pbtxt')  # pretrained net\n",
    "\n",
    "swapRB = True\n",
    "classNames = { 0: 'background',\n",
    "    1: 'person', 2: 'bicycle', 3: 'car', 4: 'motorcycle', 5: 'airplane', 6: 'bus',\n",
    "    7: 'train', 8: 'truck', 9: 'boat', 10: 'traffic light', 11: 'fire hydrant',\n",
    "    13: 'stop sign', 14: 'parking meter', 15: 'bench', 16: 'bird', 17: 'cat',\n",
    "    18: 'dog', 19: 'horse', 20: 'sheep', 21: 'cow', 22: 'elephant', 23: 'bear',\n",
    "    24: 'zebra', 25: 'giraffe', 27: 'backpack', 28: 'umbrella', 31: 'handbag',\n",
    "    32: 'tie', 33: 'suitcase', 34: 'frisbee', 35: 'skis', 36: 'snowboard',\n",
    "    37: 'sports ball', 38: 'kite', 39: 'baseball bat', 40: 'baseball glove',\n",
    "    41: 'skateboard', 42: 'surfboard', 43: 'tennis racket', 44: 'bottle',\n",
    "    46: 'wine glass', 47: 'cup', 48: 'fork', 49: 'knife', 50: 'spoon',\n",
    "    51: 'bowl', 52: 'banana', 53: 'apple', 54: 'sandwich', 55: 'orange',\n",
    "    56: 'broccoli', 57: 'carrot', 58: 'hot dog', 59: 'pizza', 60: 'donut',\n",
    "    61: 'cake', 62: 'chair', 63: 'couch', 64: 'potted plant', 65: 'bed',\n",
    "    67: 'dining table', 70: 'toilet', 72: 'tv', 73: 'laptop', 74: 'mouse',\n",
    "    75: 'remote', 76: 'keyboard', 77: 'cell phone', 78: 'microwave', 79: 'oven',\n",
    "    80: 'toaster', 81: 'sink', 82: 'refrigerator', 84: 'book', 85: 'clock',\n",
    "    86: 'vase', 87: 'scissors', 88: 'teddy bear', 89: 'hair drier', 90: 'toothbrush' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_robot_in_world_pos(robot_theta, color_image, depth_image, depth_frame, depth_intrin):\n",
    "    # detect and decode barcodes\n",
    "    object_depth = np.asanyarray(depth_frame.get_data())\n",
    "    barcode_poses = []\n",
    "    barcodes = pyzbar.decode(color_image)\n",
    "\n",
    "    bar_x = []\n",
    "    bar_y = []\n",
    "    robo_x = []\n",
    "    robo_y = []\n",
    "\n",
    "    for barcode in barcodes:\n",
    "        (x, y, w, h) = barcode.rect\n",
    "        cv2.rectangle(color_image, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "        cv2.rectangle(depth_image, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "        barcodeData = barcode.data.decode(\"utf-8\")\n",
    "        barcodeType = barcode.type\n",
    "\n",
    "        curr = copy.deepcopy(object_depth)\n",
    "        curr = curr[math.floor(((x)+(x+w))/2-1):math.ceil(((x)+(x+w))/2+1), math.floor(((y)+(y+h))/2-1):math.ceil(((y)+(y+h))/2+1)].astype(float)\n",
    "        depth_scale = profile.get_device().first_depth_sensor().get_depth_scale()\n",
    "        curr = curr * depth_scale\n",
    "        dist,_,_,_ = cv2.mean(curr)\n",
    "        barcode_poses.append((barcodeData, dist))\n",
    "\n",
    "        # draw the barcode data and barcode type on the image\n",
    "        text = \"{} ({})\".format(barcodeData, dist)\n",
    "        cv2.putText(color_image, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "\n",
    "        # store poses of codes\n",
    "        curr_pose = re.split(',|\\(|\\)',barcodeData)\n",
    "        bar_x.append(float(curr_pose[1]))\n",
    "        bar_y.append(float(curr_pose[2]))\n",
    "\n",
    "        # Get 3d point of object detected\n",
    "        relative_depth_point = rs.rs2_deproject_pixel_to_point(depth_intrin, [int((x+(x+w))/2), int((y+(y+h))/2)], dist)\n",
    "#             print(\"\\Relative Point: \\nX \",relative_depth_point[0],\"\\nY \",relative_depth_point[1],\"\\nZ \",relative_depth_point[2])\n",
    "\n",
    "        # Get world position of cam according to this barcode\n",
    "        curr_cam_theta = robot_theta\n",
    "#             print(\"Cam Theta: \\n\",curr_cam_theta)\n",
    "        curr_cam_phi = np.arctan2(relative_depth_point[2],relative_depth_point[0])*180/np.pi\n",
    "#             print(\"Cam Phi: \\n\",curr_cam_phi)\n",
    "        curr_cam_alpha = 90 - curr_cam_theta + curr_cam_phi\n",
    "#             print(\"Cam Alpha: \\n\",curr_cam_alpha)\n",
    "        curr_cam_delta_x = dist*np.cos(np.radians(curr_cam_alpha))\n",
    "#             print(\"Cam delta_x: \\n\",curr_cam_delta_x)\n",
    "        curr_cam_delta_y = dist*np.sin(np.radians(curr_cam_alpha))\n",
    "#             print(\"Cam delta_y: \\n\",curr_cam_delta_y)             \n",
    "#             print(\"\\nBarcode Pose: \\nX \",curr_pose[1],\"\\nY \",curr_pose[2])\n",
    "        robo_x.append(float(curr_pose[1])-float(curr_cam_delta_x))\n",
    "        robo_y.append(float(curr_pose[2])-float(curr_cam_delta_y))\n",
    "\n",
    "    if barcodes:\n",
    "        curr_cam_pos = [np.average(robo_x), np.average(robo_y)]\n",
    "\n",
    "        text_pos_x = 'X: ' + str(curr_cam_pos[0])\n",
    "        text_pos_y = 'Y: ' + str(curr_cam_pos[1])\n",
    "\n",
    "        cv2.putText(depth_image, text_pos_x, (20, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "        cv2.putText(depth_image, text_pos_y, (20, 420), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "        \n",
    "        return curr_cam_pos\n",
    "    \n",
    "    return None\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_human_in_world_pos(net, color_frame, depth_frame, profile, robot_theta, depth_image, color_image):\n",
    "    expected = 300\n",
    "    inScaleFactor = 0.007843\n",
    "    meanVal = 127.53\n",
    "    \n",
    "    color_human_image = np.asanyarray(color_frame.get_data())\n",
    "    \n",
    "    # crop color image for detection\n",
    "    height, width = color_human_image.shape[:2]\n",
    "    \n",
    "    aspect = width / height\n",
    "    resized_color_image = cv2.resize(color_human_image, (round(expected * aspect), expected))\n",
    "    crop_start = round(expected * (aspect - 1) / 2)\n",
    "    crop_color_img = resized_color_image[0:expected, crop_start:crop_start + expected]\n",
    "\n",
    "    # Perform object detection through net\n",
    "    blob = cv2.dnn.blobFromImage(crop_color_img, inScaleFactor, (expected, expected), meanVal, False)\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward(\"detection_out\")\n",
    "\n",
    "    label = detections[0, 0, 0, 1]\n",
    "    conf = detections[0, 0, 0, 2]\n",
    "    xmin = detections[0, 0, 0, 3]\n",
    "    ymin = detections[0, 0, 0, 4]\n",
    "    xmax = detections[0, 0, 0, 5]\n",
    "    ymax = detections[0, 0, 0, 6]\n",
    "\n",
    "    if (conf >= .5):\n",
    "        className = classNames[int(label)]\n",
    "\n",
    "        # Calculate box coordinates of detected object\n",
    "        scale = height / expected\n",
    "        xmin_depth = int((xmin * expected + crop_start) * scale)\n",
    "        ymin_depth = int((ymin * expected) * scale)\n",
    "        xmax_depth = int((xmax * expected + crop_start) * scale)\n",
    "        ymax_depth = int((ymax * expected) * scale)\n",
    "        xmin_depth, ymin_depth, xmax_depth, ymax_depth\n",
    "\n",
    "        # Calculate depth of object\n",
    "        depth = np.asanyarray(depth_frame.get_data())\n",
    "        # Crop depth data:\n",
    "        depth = depth[math.floor((xmax_depth + xmin_depth) / 2 - 1):math.ceil((xmax_depth + xmin_depth) / 2 + 1), math.floor((ymax_depth + ymin_depth) / 2 - 1):math.ceil((ymax_depth + ymin_depth) / 2 + 1)].astype(float)\n",
    "\n",
    "        # Get data scale from the device and convert to meters\n",
    "        depth_scale = profile.get_device().first_depth_sensor().get_depth_scale()\n",
    "        depth = depth * depth_scale\n",
    "        dist, _, _, _ = cv2.mean(depth)\n",
    "\n",
    "        # Get relative location of person to camera\n",
    "        relative_depth_point = rs.rs2_deproject_pixel_to_point(depth_intrin, [int((xmin_depth + xmax_depth) / 2), int((ymin_depth + ymax_depth) / 2)], dist)\n",
    "\n",
    "        # Get world position of cam according to this barcode\n",
    "        curr_cam_theta = robot_theta\n",
    "#             print(\"Cam Theta: \\n\",curr_cam_theta)\n",
    "        curr_cam_phi = np.arctan2(relative_depth_point[2],relative_depth_point[0])*180/np.pi\n",
    "#             print(\"Cam Phi: \\n\",curr_cam_phi)\n",
    "        curr_cam_alpha = 90 - curr_cam_theta + curr_cam_phi\n",
    "#             print(\"Cam Alpha: \\n\",curr_cam_alpha)\n",
    "        curr_person_delta_x = dist*np.cos(np.radians(curr_cam_alpha))\n",
    "#             print(\"Cam delta_x: \\n\",curr_cam_delta_x)\n",
    "        curr_person_delta_y = dist*np.sin(np.radians(curr_cam_alpha))\n",
    "#             print(\"Cam delta_y: \\n\",curr_cam_delta_y)             \n",
    "#             print(\"\\nBarcode Pose: \\nX \",curr_pose[1],\"\\nY \",curr_pose[2])\n",
    "\n",
    "        # Draw square on depth and color streams\n",
    "        cv2.rectangle(depth_image, (xmin_depth, ymin_depth), (xmax_depth, ymax_depth), (255, 255, 255), 2)\n",
    "        cv2.rectangle(color_image, (xmin_depth, ymin_depth), (xmax_depth, ymax_depth), (255, 255, 255), 2)\n",
    "        cv2.putText(color_image, className + \" @ \" + \"{:.2f}\".format(dist) + \"meters away\", (xmin_depth, ymin_depth), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255))\n",
    "\n",
    "        return curr_person_delta_x, curr_person_delta_y, color_frame.get_timestamp()\n",
    "    \n",
    "    return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOT FIRST PERSON DATA AS: X:  0.06425923327508393  Y:  0.35003951806221323 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  0.27411638250271514  Y:  1.9286172687501195 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -0.00047805216638749587  VY:  0.000305249697141477 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.8668482166626913  Y:  3.1770303577283983 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  0.0029451461409202946  VY:  -0.010872079831080312 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.47731396253874264  Y:  1.8513483874892498 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -0.004878679176736011  VY:  0.0175571676797008 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -1.308798397513497  Y:  4.760013324302159 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -7.037138249019301e-05  VY:  8.814446970015307e-05 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -1.3863375231518373  Y:  4.756068397665997 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  9.108756517660016e-05  VY:  -9.426437205939636e-05 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -1.35720354464392  Y:  4.663523322486813 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -2.2333675284395677e-05  VY:  0.00023202887164958138 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.5592265219052217  Y:  1.965203079368937 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  5.815746509436024e-05  VY:  0.00023150929805482417 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.5417292864597164  Y:  1.9709048117741452 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  0.00016551539448746895  VY:  -0.000408868225892538 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.5074283714903598  Y:  1.9055972337357407 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  4.3061165714411994e-05  VY:  -0.000574460229539046 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.49228454879492  Y:  1.7734419908210548 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  0.00012907048509644528  VY:  -0.00031525570753964684 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.4468972394445387  Y:  1.658166733268042 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.060171302706253184  Y:  1.0910086948285223 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  0.0006508955394944002  VY:  -0.0006756122930481818 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  0.23358397010794393  Y:  3.3578855854833267 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.023509348699718775  Y:  5.075945799369922 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -0.0007390275796322703  VY:  -0.029528809587213534 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.17598598754652944  Y:  1.2220600864014224 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -0.000516448370737725  VY:  0.0005256191621016334 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.3068590811160597  Y:  1.415623521156095 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -0.00042604675005989735  VY:  0.0008464024809723413 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.41651705931349814  Y:  1.5515656719803066 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.698164539756135  Y:  2.8016532342794163 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -0.0003104450557557913  VY:  0.0007837347051924802 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.5293000155221003  Y:  2.0034262142412347 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  0.00011303466517837549  VY:  -0.0004140505559782276 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.4986387494179763  Y:  1.8913740404857267 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  4.461176348145224e-05  VY:  -0.00019565620170242722 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.49685551376216885  Y:  1.9008024821710199 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -1.938066588525978e-05  VY:  -4.1105582653557184e-05 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.5023521274470327  Y:  1.9181425237223244 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -6.464863581830581e-05  VY:  3.324928967124115e-05 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.5222737167920459  Y:  1.9395804509753933 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  6.666486343408464e-06  VY:  -9.836688350449912e-05 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.5365798445936636  Y:  1.9607956796982247 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -2.9012435586412865e-05  VY:  -1.9680995363973933e-05 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.5408161420405577  Y:  1.955251067229793 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -2.689407829473644e-05  VY:  9.723214600538033e-05 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.5485028798506201  Y:  1.9714525354915247 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  5.1641010317300314e-05  VY:  -0.00018560972811101776 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.5323376570117472  Y:  1.9413361388133308 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -0.00011365825722573517  VY:  0.00019599917598645168 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.5301467281229054  Y:  1.9097824011733229 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  3.571363960861182e-05  VY:  -0.00027133428468664625 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.5159151233728613  Y:  1.8036651351617223 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  3.96091181379925e-05  VY:  -0.00042205143928840096 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.4943061273910362  Y:  1.7117236532456237 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  0.00024301075854578206  VY:  -0.000706301607126981 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.4303156853767778  Y:  1.5244292864105735 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.06069410925741234  Y:  0.6598813362472062 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  0.00011462321623400901  VY:  -0.0001024938936824431 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.040016062754416486  Y:  0.6342389164079264 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.21721443608982552  Y:  1.0312612590351422 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -0.00023331059292087155  VY:  0.0006673302089646061 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.3028332252454632  Y:  1.2755442763609457 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -0.00030295729006779223  VY:  0.0006006518287172115 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.3730279580002866  Y:  1.4177469261857019 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -0.0003064378243554456  VY:  0.0003579215491342549 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -1.4302514166021532  Y:  4.784811308137854 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  7.809980326189804e-05  VY:  -0.00041398405429890104 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -1.410526203469188  Y:  4.72182190857465 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  0.00012669063899058456  VY:  3.753669344900915e-05 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -1.3737878103590702  Y:  4.719103129506933 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -0.00027266779636827773  VY:  0.0005363008421273275 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -1.4057017283964357  Y:  4.723260416042294 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -0.0003060340446877804  VY:  -5.210399736000831e-05 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -1.4750293974887667  Y:  4.720234356210257 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  5.915533746011625e-05  VY:  -0.00021817777812696676 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -1.4582270862330469  Y:  4.666465154222732 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  2.889813209214668e-05  VY:  0.00017250522766467679 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -1.4147661966399734  Y:  4.706980338142295 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -0.00013490911843136972  VY:  0.0004185458573126081 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.8424436578667762  Y:  2.7853885703338412 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -0.0035779841955531806  VY:  0.011496120283891393 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -1.4366374236746933  Y:  4.727531727239241 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  3.451030399471035e-05  VY:  0.00026919642774351875 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -1.38559847692955  Y:  4.641948355429442 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  2.0529077840767756e-05  VY:  0.00020578185749945663 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -1.3981947880112793  Y:  4.734525470165525 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -6.732948580682901e-05  VY:  0.0004238043140679257 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -1.3904990883630872  Y:  4.734533223911301 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -0.0001514984092301268  VY:  -0.00013987207739640957 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -1.4703596928382237  Y:  4.710343399024661 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -0.0005045953301162317  VY:  -0.00038343019606347403 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -1.534988938313098  Y:  4.673719257995971 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -4.1083056530849385e-05  VY:  6.876260315886413e-05 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -1.4673298434783866  Y:  4.690860307582287 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -0.00026722800230994493  VY:  -8.471014303564555e-05 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -1.5109977947540103  Y:  4.7042926124343944 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  3.828378361673057e-05  VY:  -0.00026111056684151047 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -1.4940127581756477  Y:  4.682430263572719 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  4.986754542163264e-05  VY:  1.5862395631844263e-05 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOT FIRST PERSON DATA AS: X:  -1.4954642883999594  Y:  4.671730558405329 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -0.00024717069274828566  VY:  0.0006189153758634256 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -1.5153853876883567  Y:  4.666445777171945 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  0.00023845876885368497  VY:  -5.50690012484183e-06 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -1.4725837239375463  Y:  4.7164584695670255 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -2.127216093127244e-05  VY:  -0.0002111704883608084 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -1.5394443503433732  Y:  4.708739685263122 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -1.471697609556145  Y:  4.655417775086556 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  0.00019507360868086126  VY:  -0.000288261446377009 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -1.4592802688359279  Y:  4.734196809212954 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  0.00011481026192514849  VY:  -0.00037246676120644955 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -1.4229304139650005  Y:  4.745248925861463 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.29980036188317105  Y:  1.3381610638368555 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  9.142935984413964e-05  VY:  0.00010792919816746923 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.21768069612872648  Y:  1.4254751180086933 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -0.0006664844170819782  VY:  0.00721301762513301 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.16917473164057695  Y:  2.847312088176996 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  0.0007247516097035084  VY:  -0.001255400328305536 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  0.0007975264138373164  Y:  2.131555507600843 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  0.23548965372335615  Y:  1.9978357753310645 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  0.00024518160819851105  VY:  -0.0004352976615392599 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  0.05784041894042526  Y:  0.3590375230315682 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  2.4946919525553585e-05  VY:  -8.539388073313669e-05 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  0.06523769864299334  Y:  0.3199160871266661 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  1.8069498201788396e-05  VY:  -0.0001264339751328568 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  0.06814990933835678  Y:  0.2971177540900757 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -2.7456880691858246e-05  VY:  0.00018472085299023986 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  0.06257880107572744  Y:  0.34232719021454827 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -4.48806842521326e-05  VY:  0.00012086740598256187 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  0.24569809878536697  Y:  2.02011332297189 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -0.0004277455285877104  VY:  -0.00013501339299444378 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  0.14396260511434356  Y:  2.007845649760982 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -0.00031613003553677694  VY:  -5.345469311589866e-05 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.02020839540758044  Y:  2.182906565442558 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -0.0007434071791148262  VY:  0.003912716229876946 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.19434567281748852  Y:  2.781385241034868 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.20643587882782488  Y:  1.515000148519982 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -0.000535206140618493  VY:  -0.00024268977993443993 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.37074476440691184  Y:  1.4307454445776555 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -0.00015501372697778877  VY:  1.3802128285717867e-05 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -1.4345085385802916  Y:  4.7417616540813405 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -0.00018899796434424817  VY:  -0.0005747940773714409 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -1.4752461941812571  Y:  4.647475328077955 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -0.0002236764574960209  VY:  8.183291130399072e-05 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -1.508494774635487  Y:  4.718745791656233 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  0.00019576894536033298  VY:  -0.00018309287739464195 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -1.4827675963787608  Y:  4.686003366403234 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -0.00016594730858751012  VY:  -5.304976264369277e-05 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -1.477594881789554  Y:  4.728512630488078 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  0.006051243395014028  VY:  -0.018768521494721597 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.4220757207855992  Y:  1.5547261278005167 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  7.54134192535375e-05  VY:  6.955815419662937e-05 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.42076378513993296  Y:  1.5333160395924041 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  0.00029567982852368247  VY:  1.450570580221427e-05 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.28855229530047977  Y:  1.5324027696083438 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  0.0004721586776207244  VY:  0.0002878872994425608 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.13122849621970512  Y:  1.6133384977902534 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -3.9068326933563696e-05  VY:  0.008572697383785884 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.09551181659223959  Y:  3.265436959194535 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  0.0002705175589156044  VY:  -0.00437385114609552 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  0.045229244523300294  Y:  2.043499623763725 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  0.0005537041983363692  VY:  -0.00028946949856114115 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  0.1906279579546976  Y:  2.013245249001526 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -0.0008124396483804528  VY:  -0.009932412051237311 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  0.061883793180201846  Y:  0.3429051451050193 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  7.0173143376202775e-06  VY:  -6.729999691826549e-05 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  0.06526162279321676  Y:  0.31599795609853565 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  1.4335518980914791e-05  VY:  -0.0001108238035567356 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  0.06704905508068781  Y:  0.3019793774642335 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -2.5436410031521433e-05  VY:  0.0001573660444838823 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  0.05656209832672358  Y:  0.3589036816255471 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  0.0009171986173178796  VY:  0.009874239578709405 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  0.1413874141417999  Y:  1.9990062482702131 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -0.0007287146798637027  VY:  0.0006378764990841962 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.08710009977980908  Y:  2.5496239118928314 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -0.0003364076856120501  VY:  -0.002637802662240434 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.1770251063398417  Y:  1.662825768941541 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -0.00031464224171262546  VY:  -0.0006043294861581347 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.23844961363521963  Y:  1.495104678443672 \n",
      "\n",
      "GOT PERSON VELOCITY DATA AS: VX:  -0.00041226015051617023  VY:  -0.0002487740529992553 \n",
      "\n",
      "GOT FIRST PERSON DATA AS: X:  -0.27956532652116983  Y:  1.4441898884014461 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# State Representation\n",
    "robot_x = 0\n",
    "robot_y = 0\n",
    "robot_theta = 90\n",
    "robot_vx = 0\n",
    "robot_vy = 0\n",
    "human_x = 0\n",
    "human_y = 0\n",
    "human_theta = 0  # doesn't really matter\n",
    "human_vx = 0\n",
    "human_vy = 0\n",
    "goal_x = 0  # set by user\n",
    "goal_y = 0  # set by user\n",
    "time_from_last_step = 0\n",
    "\n",
    "# Start streaming\n",
    "algo = rotation_estimator()\n",
    "profile = pipeline.start(config)\n",
    "first_run = True\n",
    "\n",
    "# print(\"STARTED PIPELINE\\n\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        if first_run:\n",
    "#             print(\"THINKS ITS THE FIRST RUN\\n\")\n",
    "            try:\n",
    "                frames = pipeline.wait_for_frames()\n",
    "                \n",
    "                accel_frame, accel_data, gyro_frame, gyro_data, gyro_ts = get_robot_euler_angle(frames)\n",
    "                if not accel_frame or not gyro_frame:\n",
    "#                     print(\"FIRST NOT FOUND\")\n",
    "                    break\n",
    "                    \n",
    "                algo.process_gyro(gyro_data, gyro_ts)\n",
    "                algo.process_accel(accel_data)\n",
    "                first_run = False\n",
    "#                 print(\"FIRST PROCESSED\")\n",
    "                continue\n",
    "\n",
    "            except:\n",
    "                print(\"EXCEPTION\")\n",
    "                break\n",
    "\n",
    "#         print(\"NOT FIRST RUN ANY MORE\\n\")\n",
    "                \n",
    "        # Wait for a coherent pair of frames: depth and color\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        frames = align.process(frames)\n",
    "\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        color_frame = frames.get_color_frame()\n",
    "\n",
    "        accel_frame, accel_data, gyro_frame, gyro_data, gyro_ts = get_robot_euler_angle(frames)\n",
    "\n",
    "        if gyro_frame:\n",
    "            algo.process_gyro(gyro_data, gyro_ts)\n",
    "        if accel_frame:\n",
    "            algo.process_accel(accel_data)\n",
    "            \n",
    "        robot_theta = algo.get_theta()[1]\n",
    "\n",
    "        if not depth_frame or not color_frame:\n",
    "            # @TODO GET NEXT ACTION BASED ON JUST THE LAST STATE'S VALUES OR MAYBE STOP IN CURR POSITION\n",
    "            print(\"DIDNT GET DEPTH OR COLOR FRAME\\n\")\n",
    "            break\n",
    "\n",
    "#         print(\"GOT DEPTH AND COLOR FRAME\\n\")\n",
    "            \n",
    "        # filter depth stream: depth2disparity -> spatial -> disparity2depth -> hole_filling\n",
    "        depth_frame = depth_to_disparity.process(depth_frame)\n",
    "        depth_frame = spatial.process(depth_frame)\n",
    "        depth_frame = disparity_to_depth.process(depth_frame)\n",
    "        depth_frame = hole_filling.process(depth_frame)\n",
    "\n",
    "        # get intrinsics\n",
    "        depth_intrin = depth_frame.profile.as_video_stream_profile().intrinsics\n",
    "        color_intrin = color_frame.profile.as_video_stream_profile().intrinsics\n",
    "\n",
    "        # Convert images to numpy arrays\n",
    "        depth_image = np.asanyarray(colorizer.colorize(depth_frame).get_data())\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "        curr_cam_pos = get_robot_in_world_pos(robot_theta, color_image, depth_image, depth_frame, depth_intrin)\n",
    "        if curr_cam_pos:\n",
    "            robot_x = curr_cam_pos[0]\n",
    "            robot_y = curr_cam_pos[1]\n",
    "        \n",
    "        # FIND POSITION OF PERSON\n",
    "        person_first_scan = None\n",
    "        person_second_scan = None\n",
    "        found_first = False\n",
    "        found_second = False\n",
    "\n",
    "        curr_person_delta_x, curr_person_delta_y, frame_timestamp = get_human_in_world_pos(net, color_frame, depth_frame, profile, robot_theta, depth_image, color_image)\n",
    "        \n",
    "        if curr_person_delta_x and curr_person_delta_y:\n",
    "            person_first_scan = [(float(robot_x + curr_person_delta_x)), (float(robot_y + curr_person_delta_y)), frame_timestamp]\n",
    "            \n",
    "            human_x = person_first_scan[0]\n",
    "            human_y = person_first_scan[1]\n",
    "            found_first = True                                            \n",
    "\n",
    "            # Stack both images horizontally\n",
    "            images = np.hstack((color_image, depth_image))\n",
    "\n",
    "            # Show images\n",
    "            cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE)\n",
    "            cv2.imshow('RealSense', images)\n",
    "            key = cv2.waitKey(1)\n",
    "\n",
    "            # Press esc or 'q' to close the image window\n",
    "            if key & 0xFF == ord('q') or key == 27:\n",
    "                cv2.destroyAllWindows()\n",
    "                break\n",
    "\n",
    "            print(\"GOT FIRST PERSON DATA AS: X: \", human_x, \" Y: \", human_y, \"\\n\")\n",
    "\n",
    "            # Wait for a coherent pair of frames: depth and color\n",
    "            frames = pipeline.wait_for_frames()\n",
    "            frames = align.process(frames)\n",
    "\n",
    "            depth_frame = frames.get_depth_frame()\n",
    "            color_frame = frames.get_color_frame()\n",
    "\n",
    "            accel_frame, accel_data, gyro_frame, gyro_data, gyro_ts = get_robot_euler_angle(frames)\n",
    "\n",
    "            if gyro_frame:\n",
    "                algo.process_gyro(gyro_data, gyro_ts)\n",
    "            if accel_frame:\n",
    "                algo.process_accel(accel_data)\n",
    "            \n",
    "            robot_theta = algo.get_theta()[1]\n",
    "            \n",
    "            if not depth_frame or not color_frame:\n",
    "                # @TODO GET NEXT ACTION BASED ON JUST THE LAST STATE'S VALUES OR MAYBE STOP IN CURR POSITION\n",
    "                print(\"DIDNT GET DEPTH OR COLOR FRAME\\n\")\n",
    "                break\n",
    "            \n",
    "            # filter depth stream: depth2disparity -> spatial -> disparity2depth -> hole_filling\n",
    "            depth_frame = depth_to_disparity.process(depth_frame)\n",
    "            depth_frame = spatial.process(depth_frame)\n",
    "            depth_frame = disparity_to_depth.process(depth_frame)\n",
    "            depth_frame = hole_filling.process(depth_frame)\n",
    "\n",
    "            # get intrinsics\n",
    "            depth_intrin = depth_frame.profile.as_video_stream_profile().intrinsics\n",
    "            color_intrin = color_frame.profile.as_video_stream_profile().intrinsics\n",
    "\n",
    "            # Convert images to numpy arrays\n",
    "            depth_image = np.asanyarray(colorizer.colorize(depth_frame).get_data())\n",
    "            color_image = np.asanyarray(color_frame.get_data())\n",
    "            \n",
    "            curr_person_delta_x, curr_person_delta_y, frame_timestamp = get_human_in_world_pos(net, color_frame, depth_frame, profile, robot_theta, depth_image, color_image)\n",
    "            \n",
    "            if curr_person_delta_x and curr_person_delta_y:\n",
    "                person_second_scan = [(float(robot_x + curr_person_delta_x)), (float(robot_y + curr_person_delta_y)), frame_timestamp]\n",
    "\n",
    "                human_x = person_second_scan[0]\n",
    "                human_y = person_second_scan[1]\n",
    "                found_second = True\n",
    "                \n",
    "                # Stack both images horizontally\n",
    "                images = np.hstack((color_image, depth_image))\n",
    "\n",
    "                # Show images\n",
    "                cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE)\n",
    "                cv2.imshow('RealSense', images)\n",
    "                key = cv2.waitKey(1)\n",
    "\n",
    "                # Press esc or 'q' to close the image window\n",
    "                if key & 0xFF == ord('q') or key == 27:\n",
    "                    cv2.destroyAllWindows()\n",
    "                    break\n",
    "\n",
    "        if found_first and found_second:\n",
    "            human_vx = (person_second_scan[0] - person_first_scan[0]) / (person_second_scan[2] - person_first_scan[2])\n",
    "            human_vy = (person_second_scan[1] - person_first_scan[1]) / (person_second_scan[2] - person_first_scan[2])\n",
    "            print(\"GOT PERSON VELOCITY DATA AS: VX: \", human_vx, \" VY: \", human_vy, \"\\n\")\n",
    "            \n",
    "        elif found_first and not found_second:\n",
    "            human_vx = 0\n",
    "            human_vy = 0\n",
    "        \n",
    "        else:\n",
    "            human_x = None\n",
    "            human_y = None\n",
    "            human_vx = None\n",
    "            human_vy = None\n",
    "        \n",
    "        if human_x and human_y:\n",
    "            text_pos_x = 'human X: ' + str(human_x)\n",
    "            text_pos_y = 'human Y: ' + str(human_y)\n",
    "            cv2.putText(depth_image, text_pos_x, (20, 320), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "            cv2.putText(depth_image, text_pos_y, (20, 340), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "        if human_vx and human_vy:\n",
    "            text_pos_x = 'human VX: ' + str(human_vx)\n",
    "            text_pos_y = 'human VY: ' + str(human_vy)\n",
    "            cv2.putText(depth_image, text_pos_x, (20, 360), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "            cv2.putText(depth_image, text_pos_y, (20, 380), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "        \n",
    "        # Stack both images horizontally\n",
    "        images = np.hstack((color_image, depth_image))\n",
    "\n",
    "        # Show images\n",
    "        cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE)\n",
    "        cv2.imshow('RealSense', images)\n",
    "        key = cv2.waitKey(1)\n",
    "\n",
    "        # Press esc or 'q' to close the image window\n",
    "        if key & 0xFF == ord('q') or key == 27:\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "            \n",
    "        # @TODO get ORCA decision from robot_x/y, robot_vx/vy, robot_theta, human_x/y, human_vx/vy, goal_x/y\n",
    "\n",
    "        # @TODO set robot_vy/vy & theta in ROS, and act\n",
    "\n",
    "finally:\n",
    "    # Stop streaming\n",
    "    pipeline.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}