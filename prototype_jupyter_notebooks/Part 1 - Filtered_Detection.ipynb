{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The goal of this notebook is to detect objects through the Intel RealSense camera, and output a stream showing boxes (and distances) of the detected objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RealSense Configuration Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure depth and color streams\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30) # depth stream\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30) # color stream\n",
    "align = rs.align(rs.stream.color) # align both streams to same pov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable visualizer and filters for later use\n",
    "colorizer = rs.colorizer()\n",
    "spatial = rs.spatial_filter()\n",
    "spatial.set_option(rs.option.filter_magnitude, 5)\n",
    "spatial.set_option(rs.option.filter_smooth_alpha, 1)\n",
    "spatial.set_option(rs.option.filter_smooth_delta, 50)\n",
    "spatial.set_option(rs.option.holes_fill, 3)\n",
    "hole_filling = rs.hole_filling_filter()\n",
    "depth_to_disparity = rs.disparity_transform(True)\n",
    "disparity_to_depth = rs.disparity_transform(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV Detection Configration Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image detection size\n",
    "expected = 300\n",
    "inScaleFactor = 0.007843\n",
    "meanVal = 127.53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNetFromTensorflow('frozen_inference_graph.pb', 'graph.pbtxt') # pretrained net\n",
    "\n",
    "swapRB = True\n",
    "classNames = { 0: 'background',\n",
    "    1: 'person', 2: 'bicycle', 3: 'car', 4: 'motorcycle', 5: 'airplane', 6: 'bus',\n",
    "    7: 'train', 8: 'truck', 9: 'boat', 10: 'traffic light', 11: 'fire hydrant',\n",
    "    13: 'stop sign', 14: 'parking meter', 15: 'bench', 16: 'bird', 17: 'cat',\n",
    "    18: 'dog', 19: 'horse', 20: 'sheep', 21: 'cow', 22: 'elephant', 23: 'bear',\n",
    "    24: 'zebra', 25: 'giraffe', 27: 'backpack', 28: 'umbrella', 31: 'handbag',\n",
    "    32: 'tie', 33: 'suitcase', 34: 'frisbee', 35: 'skis', 36: 'snowboard',\n",
    "    37: 'sports ball', 38: 'kite', 39: 'baseball bat', 40: 'baseball glove',\n",
    "    41: 'skateboard', 42: 'surfboard', 43: 'tennis racket', 44: 'bottle',\n",
    "    46: 'wine glass', 47: 'cup', 48: 'fork', 49: 'knife', 50: 'spoon',\n",
    "    51: 'bowl', 52: 'banana', 53: 'apple', 54: 'sandwich', 55: 'orange',\n",
    "    56: 'broccoli', 57: 'carrot', 58: 'hot dog', 59: 'pizza', 60: 'donut',\n",
    "    61: 'cake', 62: 'chair', 63: 'couch', 64: 'potted plant', 65: 'bed',\n",
    "    67: 'dining table', 70: 'toilet', 72: 'tv', 73: 'laptop', 74: 'mouse',\n",
    "    75: 'remote', 76: 'keyboard', 77: 'cell phone', 78: 'microwave', 79: 'oven',\n",
    "    80: 'toaster', 81: 'sink', 82: 'refrigerator', 84: 'book', 85: 'clock',\n",
    "    86: 'vase', 87: 'scissors', 88: 'teddy bear', 89: 'hair drier', 90: 'toothbrush' }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_layers(net):\n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    return output_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Stream Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start streaming\n",
    "profile = pipeline.start(config)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "\n",
    "        # Wait for a coherent pair of frames: depth and color\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        frames = align.process(frames)\n",
    "        \n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        \n",
    "        if not depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "        # filter depth stream: depth2disparity -> spatial -> disparity2depth -> hole_filling\n",
    "        depth_frame = depth_to_disparity.process(depth_frame)\n",
    "        depth_frame = spatial.process(depth_frame)\n",
    "        depth_frame = disparity_to_depth.process(depth_frame)\n",
    "        depth_frame = hole_filling.process(depth_frame)\n",
    "\n",
    "        # Convert images to numpy arrays\n",
    "        depth_image = np.asanyarray(colorizer.colorize(depth_frame).get_data())\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        \n",
    "        # crop color image for detection\n",
    "        height, width = color_image.shape[:2]\n",
    "        expected = 300\n",
    "        aspect = width / height\n",
    "        resized_color_image = cv2.resize(color_image, (round(expected * aspect), expected))\n",
    "        crop_start = round(expected * (aspect - 1) / 2)\n",
    "        crop_color_img = resized_color_image[0:expected, crop_start:crop_start+expected]\n",
    "\n",
    "        # Perform object detection through net\n",
    "        blob = cv2.dnn.blobFromImage(crop_color_img, inScaleFactor, (expected, expected), meanVal, False)\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward(\"detection_out\")\n",
    "\n",
    "        label = detections[0,0,0,1]\n",
    "        conf  = detections[0,0,0,2]\n",
    "        xmin  = detections[0,0,0,3]\n",
    "        ymin  = detections[0,0,0,4]\n",
    "        xmax  = detections[0,0,0,5]\n",
    "        ymax  = detections[0,0,0,6]\n",
    "        \n",
    "        if (conf < .5):\n",
    "            # Stack both images horizontally\n",
    "            images = np.hstack((color_image, depth_image))\n",
    "\n",
    "            # Show images\n",
    "            cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE)\n",
    "            cv2.imshow('RealSense', images)\n",
    "            key = cv2.waitKey(1)\n",
    "\n",
    "            # Press esc or 'q' to close the image window\n",
    "            if key & 0xFF == ord('q') or key == 27:\n",
    "                cv2.destroyAllWindows()\n",
    "                break\n",
    "            \n",
    "            continue\n",
    "        \n",
    "        className = classNames[int(label)]\n",
    "\n",
    "        # Calculate box coordinates of detected object\n",
    "        scale = height / expected\n",
    "        xmin_depth = int((xmin * expected + crop_start) * scale)\n",
    "        ymin_depth = int((ymin * expected) * scale)\n",
    "        xmax_depth = int((xmax * expected + crop_start) * scale)\n",
    "        ymax_depth = int((ymax * expected) * scale)\n",
    "        xmin_depth,ymin_depth,xmax_depth,ymax_depth\n",
    "        \n",
    "        # Calculate depth of object\n",
    "        depth = np.asanyarray(depth_frame.get_data())\n",
    "        # Crop depth data:\n",
    "        depth = depth[math.floor((xmax_depth+xmin_depth)/2-1):math.ceil((xmax_depth+xmin_depth)/2+1),math.floor((ymax_depth+ymin_depth)/2-1):math.ceil((ymax_depth+ymin_depth)/2+1)].astype(float)\n",
    "\n",
    "        # Get data scale from the device and convert to meters\n",
    "        depth_scale = profile.get_device().first_depth_sensor().get_depth_scale()\n",
    "        depth = depth * depth_scale\n",
    "        dist,_,_,_ = cv2.mean(depth)        \n",
    "        \n",
    "        # Draw square on depth and color streams\n",
    "        cv2.rectangle(depth_image, (xmin_depth, ymin_depth), \n",
    "            (xmax_depth, ymax_depth), (255, 255, 255), 2)\n",
    "        cv2.rectangle(color_image, (xmin_depth, ymin_depth), \n",
    "            (xmax_depth, ymax_depth), (255, 255, 255), 2)\n",
    "        cv2.putText(color_image, className+\" @ \"+\"{:.2f}\".format(dist)+\"meters away\", \n",
    "            (xmin_depth, ymin_depth),\n",
    "            cv2.FONT_HERSHEY_COMPLEX, 0.5, (255,255,255))\n",
    "        \n",
    "        # Stack both images horizontally\n",
    "        images = np.hstack((color_image, depth_image))\n",
    "\n",
    "        # Show images\n",
    "        cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE)\n",
    "        cv2.imshow('RealSense', images)\n",
    "        key = cv2.waitKey(1)\n",
    "        \n",
    "        # Press esc or 'q' to close the image window\n",
    "        if key & 0xFF == ord('q') or key == 27:\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "        \n",
    "finally:\n",
    "\n",
    "    # Stop streaming\n",
    "    pipeline.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
